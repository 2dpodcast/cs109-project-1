{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topics: clustering within subreddit, upvote prediction, sentence prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import requests\n",
      "from scipy import stats\n",
      "\n",
      "from operator import itemgetter\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Predicting Upvotes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to predict upvotes for a comment on each subreddit, we separated the process into three parts:\n",
      "\n",
      "1. **Upvote Normalization.** In order to provide input to our supervised learning model, we normalized our upvotes to the interval $[0,1]$.\n",
      "2. **Feature Selection.** We used different types of features to summarize text within a comment. These features include using a Bag of Words model, N-gram model, Co-occurence model, and Latent Dirichlet Allocation.\n",
      "3. **Dimensionality Reduction.** Because the dimensionality of the feature space can be extremely large, we had to reduce the dimensionality of the feature space to make the training phrase tractable. To do this, we used multiple ways, including: Principle Components Analysis, Selecting k best features.\n",
      "4. **Learning.** We used several supervised learning methods using our features $X$ and the normalized number of upvotes $Y$ as the target value for our regressions. Our supervised learning methods include: Gaussian Naive Bayes, Support Vector Machines, and K Nearest Neighbors.\n",
      "5. **Putting it all together.** We experimented with different combinations of feature selection, dimensionality reduction, and learning models and discuss our results for each.\n",
      "\n",
      "\n",
      "Below, we include some utility functions for reading in subreddit data.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Data format:\n",
      "\n",
      "{\n",
      "  \"body\": string\n",
      "  \"post_ups\": int\n",
      "  \"subreddit_id\": string\n",
      "  \"created\": float (timestamp)\n",
      "  \"downs\": int\n",
      "  \"author\": string\n",
      "  \"post_net\": int\n",
      "  \"subreddit\": string\n",
      "  \"post_id\": string\n",
      "  \"post_downs\": int\n",
      "  \"net\": int\n",
      "  \"ups\": int\n",
      "  \"id\": string\n",
      "  \"post_created\": float\n",
      "}\n",
      "\"\"\"\n",
      "FIELDS = [\"body\", \"post_ups\", \"subreddit_id\", \"created\", \"downs\",\n",
      "          \"author\", \"post_net\", \"subreddit\", \"post_id\", \"post_downs\",\n",
      "          \"net\", \"ups\", \"id\", \"post_created\"]\n",
      "\n",
      "def load_subreddit(filename, fields=FIELDS):\n",
      "    \"\"\"\n",
      "    Loads the subreddit with the filename and returns\n",
      "    a dataframe where the column names are the fields\n",
      "    in the json object.\n",
      "    \"\"\"\n",
      "    file = open(filename, \"rb\")\n",
      "    arrays = {field:[] for field in fields}\n",
      "    for line in file.readlines():\n",
      "        data = json.loads(line)\n",
      "        for field in fields:\n",
      "            arrays[field].append(data[field])\n",
      "    df = pd.DataFrame(arrays)\n",
      "    file.close()\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Upvote Normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To provide input to our supervised learning model, we must normalize our upvotes to the interval $[0,1]$. To do this, we used the following procedures to normalize and denormalize upvotes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topscores = {'Liberal': 106, 'videos': 10341, 'gentlemanboners': 1619, 'books':\n",
      "        4914, 'Music': 7286, 'politics': 15133, 'nba': 4108, 'pokemon': 3270,\n",
      "        'funny': 9633, 'technology': 10848, 'Conservative': 438, 'food': 3358,\n",
      "        'WTF': 11107, 'worldnews': 10559, 'soccer': 2985, 'gaming': 16413,\n",
      "        'aww': 7656, 'circlejerk': 3069, 'LadyBoners': 1190, 'news': 10995,\n",
      "        'television': 9274, 'science': 8965, 'nfl': 5416, 'pics': 19196,\n",
      "        'movies': 93504}\n",
      "\n",
      "\"\"\"\n",
      "normalize_scores\n",
      "    Normalizes the score based on the max upvotes in the given subreddit.\n",
      "\n",
      "    @param: ups (array of upvote scores), subreddit (name of subreddit)\n",
      "    @ret: array of normalized scores\n",
      "\"\"\"\n",
      "def normalize_scores(ups, subreddit):\n",
      "    return [float(x)/topscores[subreddit] for x in ups]\n",
      "\n",
      "def denormalize_scores(norms, subreddit):\n",
      "    return [x * topscores[subreddit] for x in norms]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Feature Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We abstracted the process of feature selection into an abstract class `AbstractFeatureModel`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abc import ABCMeta, abstractmethod\n",
      "import numpy as np\n",
      "from gensim import corpora\n",
      "from gensim.models import ldamodel\n",
      "from scipy.sparse import vstack\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "\n",
      "class AbstractFeatureModel(object):\n",
      "    \"\"\"\n",
      "    Interface for all feature extractors. Extend this class to\n",
      "    create new models for analyzing comment data.\n",
      "    \"\"\"\n",
      "    __metaclass__ = ABCMeta\n",
      "\n",
      "    @abstractmethod\n",
      "    def make_training_xy(self, data):\n",
      "        \"\"\"\n",
      "        Extract a feature matrix X and value vector Y from the training data set.\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        data : dataframe containing comments data and upvote data\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : numpy array (dims: ncomments x nfeatures)\n",
      "            each row of X represents the features associated with each comment\n",
      "        Y : numpy array (dims: ncomments)\n",
      "            each entry corresponds to the value associated with each comment\n",
      "            (e.g. normalized upvote score, subreddit id)\n",
      "\n",
      "        Usage\n",
      "        -----\n",
      "        model = MyModel(param1, param2)\n",
      "        X, Y = model.make_training(data)\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def data_to_x(self, new_data):\n",
      "        \"\"\"\n",
      "        Extracts a feature matrix X from the new data set (for predictions), where\n",
      "        the number of rows of X is the number of new data items in new_data\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        new_data : dataframe containing comments data, but no label data (upvotes, subreddit)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : feature matrix (num rows equal to number of entries in new_data)\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def y_to_label(self, data, Y):\n",
      "        \"\"\"\n",
      "        Translates a y value back into its true representation (e.g. the\n",
      "        denormalized upvote score, the subreddit name).\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        data : dataframe containing comments data and upvote data\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        labels : The human readable label for the given Y values, len(labels) == len(Y)\n",
      "        \"\"\"\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1a. Bag of Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the **Bag of Words model**, we split the comment into a list of words, and we represent each comment $c$ with a vector $x_c$ where the $i$-th component of $x_c$ is the number of times word $i$ appears in comment $x_c$. The size of vector $x_c$ is the total number of different words in all the comments $c$.\n",
      "\n",
      "We remove the most common English **stopwords** (is, are, be, etc.) as they appear very frequently in the English language, but do not provide us with much information about the content of the text.\n",
      "\n",
      "We also employ the [**term frequency-inverse document frequency**](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) (tfidf) which is a numerical statistic that reflects how important a word is to a document in a collection of documents. In our case, a \"document\" is a comment, and our \"collection of documents\" is the set of comments in a specific subreddit. \n",
      "\n",
      "To use tfidf, given a term $t$ and a document $d$, we define the **term frequency** $tf(t,d)$ as:\n",
      "\n",
      "$tf(t,d) \\propto \\frac{f(t,d)}{max\\{ f(w,d) : w \\in d\\}}$\n",
      "\n",
      "where $f(t,d)$ is the frequency of the term $t$ in document $d$. We normalize by the maximum frequency of a any word in a document to prevent biases for longer documents.\n",
      "\n",
      "We also define the **inverse document frequency** $idf(t, D)$ for a term $t$ and collection of documents $D$ as:\n",
      "\n",
      "$tfidf(t, D) = \\log \\frac{|D|}{|\\{d \\in D : t\\in d\\}|}$\n",
      "\n",
      "which measures how common the term $t$ is across all documents $D$. Finally, we define the **term frequency-inverse document frequency** $tfidf(t, d, D)$ for a term $t$, document $d$, and collection of documents $D$ as:\n",
      "\n",
      "$tfidf(t, d, D) = tf(t, d) \\times idf(t, D)$\n",
      "\n",
      "which is high for words that have a high term frequency and is not common across the collection of documents. This statistic thus gives low weight to common terms (which tend to be common English words).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BagOfWordsModel(AbstractFeatureModel):\n",
      "    \"\"\"\n",
      "    Bag of words model. This is only an example.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, min_df=0, tfidf=True):\n",
      "        self.vectorizer = CountVectorizer(min_df=min_df, stop_words='english')\n",
      "        self.tfidf = tfidf\n",
      "\n",
      "    def make_training_xy(self, data):\n",
      "        X = self.vectorizer.fit_transform(data.body)\n",
      "        if self.tfidf:\n",
      "            X = TfidfTransformer().fit_transform(X)\n",
      "        X = X.tocsc()\n",
      "        Y = normalize_scores(data.ups, data.subreddit[0])\n",
      "        return X,Y\n",
      "\n",
      "    def data_to_x(self, new_data):\n",
      "        return self.vectorizer.transform(new_data.body)\n",
      "\n",
      "    def y_to_label(self, data, Y):\n",
      "        return denormalize_scores(Y, data.subreddit[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1b. N-gram Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another model we used to extract features from text is the **N-gram** model, which generalizes the Bag of Words model to look for sequences of $N$ words that appear in text. For example, the 2-grams for the sentence, `\"he ran away quickly\"` would be the list:\n",
      "`[\"he ran\", \"ran away\", \"away quickly\"]`. \n",
      "\n",
      "Similarly to the bag of words model, we remove English stop words and apply the tfidf technique."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NGramModel(AbstractFeatureModel):\n",
      "    \"\"\"\n",
      "    n-gram model for analyzing text\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, n, min_df=0, tfidf=True):\n",
      "        self.n = n\n",
      "        self.tfidf = tfidf\n",
      "        self.vectorizer = CountVectorizer(ngram_range=(n,n), min_df=min_df,\n",
      "                                          stop_words='english')\n",
      "\n",
      "    def make_training_xy(self, data):\n",
      "        X = self.vectorizer.fit_transform(data.body)\n",
      "        if self.tfidf:\n",
      "            X = TfidfTransformer().fit_transform(X)\n",
      "        X = X.tocsc()\n",
      "        Y = normalize_scores(data.ups, data.subreddit[0])\n",
      "        return X, Y\n",
      "\n",
      "    def data_to_x(self, new_data):\n",
      "        return self.vectorizer.transform(new_data.body)\n",
      "\n",
      "    def y_to_label(self, data, Y):\n",
      "        return denormalize_scores(Y, data.subreddit[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1c. Co-occurence Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: talk about math representation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1d. Latent Dirichlet Allocation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: talk about math"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, to understand why dimensionality reduction is necessary to make our computation tractable, we will print out the number of features generated by our various models for feature selection:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_file = \"../data/Liberal\"\n",
      "df = load_subreddit(data_file)\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 4723 entries, 0 to 4722\n",
        "Data columns (total 14 columns):\n",
        "author          4723  non-null values\n",
        "body            4723  non-null values\n",
        "created         4723  non-null values\n",
        "downs           4723  non-null values\n",
        "id              4723  non-null values\n",
        "net             4723  non-null values\n",
        "post_created    4723  non-null values\n",
        "post_downs      4723  non-null values\n",
        "post_id         4723  non-null values\n",
        "post_net        4723  non-null values\n",
        "post_ups        4723  non-null values\n",
        "subreddit       4723  non-null values\n",
        "subreddit_id    4723  non-null values\n",
        "ups             4723  non-null values\n",
        "dtypes: float64(2), int64(6), object(6)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_dimension_size(df, model):\n",
      "    X_train, Y_train = model.make_training_xy(df)\n",
      "    return X_train.shape[1]\n",
      "\n",
      "models = {\n",
      "    \"bag of words\": BagOfWordsModel(),\n",
      "    \"2-gram\": NGramModel(2),\n",
      "    \"3-gram\": NGramModel(3),\n",
      "    \"4-gram\": NGramModel(4),\n",
      "}\n",
      "for name, model in models.items():\n",
      "    dim = feature_dimension_size(df, model)\n",
      "    print \"size of feature space (%s): %d\" % (name, dim)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of feature space (2-gram): 78777\n",
        "size of feature space (4-gram): 83415"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "size of feature space (3-gram): 86205"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "size of feature space (bag of words): 13588"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, the feature space is extremely large. Combined with a large number of comments, the memory footprint of training a learner without reducing the dimensionality will be intractable for most commodity machines. Thus, we need to reduce the dimensionality of our feature space, while still maintaining enough information to make useful upvote predictions.\n",
      "\n",
      "We abstracted our reduction strategies into an abstract bas class `AbstractReduction`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abc import ABCMeta, abstractmethod\n",
      "from sklearn.decomposition import KernelPCA\n",
      "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "class AbstractReduction(object):\n",
      "    \"\"\"\n",
      "    Interface for dimensionality reduction. Extend this for new models for\n",
      "    reducing dimensionality of a dataset.\n",
      "\n",
      "    Usage\n",
      "    -----\n",
      "    reducer = MyReduction(...)\n",
      "    reducer.fit(X) # call this only once!\n",
      "    X_reduced = reducer.transform(X)\n",
      "    other_x_reduced = reducer.transform(other_x)\n",
      "    \"\"\"\n",
      "    __metaclass__ = ABCMeta\n",
      "\n",
      "    @abstractmethod\n",
      "    def n_components(self):\n",
      "        \"\"\"\n",
      "        Return the target reduced dimension size.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def fit(self, X, Y=None):\n",
      "        \"\"\"\n",
      "        Fit the estimator to this data set. This must be called exactly ONCE\n",
      "        to have consistent results when we call transform().\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        X : feature matrix (n_comments x n_features)\n",
      "        Y : values to fit to\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Nothing\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def transform(self, X):\n",
      "        \"\"\"\n",
      "        Transforms the feature matrix X into a feature matrix of smaller size\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        X : feature matrix (n_comments x n_features)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_reduced : reduced feature matrix (n_comments x n_components)\n",
      "        \"\"\"\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3a. Select K Best"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For our first attempt, we will pick the best $k$ features with the highest scores, where score is computed by computing linear regressions on our input vectors $x_c$ and $y_c$ where $x_c$ is the feature vector and $y_c$ is the normalized upvote score for a comment $c$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SelectKBestReduction(AbstractReduction):\n",
      "    \"\"\"\n",
      "    Select K Best features using linear regressions\n",
      "\n",
      "    http://stackoverflow.com/questions/10098533/implementing-bag-of-words-naive-bayes-classifier-in-nltk\n",
      "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
      "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, n_components, score_func=lambda X, y: f_regression(X, y, center=False)):\n",
      "        self.select = SelectKBest(score_func=score_func, k=n_components)\n",
      "        self.n_components = n_components\n",
      "\n",
      "    def n_components(self):\n",
      "        return self.n_components\n",
      "\n",
      "    def fit(self, X, Y=None):\n",
      "        self.select.fit(X, Y)\n",
      "\n",
      "    def transform(self, X):\n",
      "        return self.select.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3b. Kernel Principle Components Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also use **Kernel Principle Components Analysis**, a generalization of linear PCA\n",
      "to reduce the dimensionality of our feature space $X$. As a review of PCA, given\n",
      "a feature space $\\mathbf{X}$ with dimensionality $d$, our goal is to compute a mapping\n",
      "from $\\mathbf{X}$ to a new feature space $\\mathbf{X'}$ with dimensionality $d' \\leq d$.\n",
      "\n",
      "Linear PCA does this by choosing the principle $d'$ eigenvectors of  the original feature space $X$\n",
      "that **preserves maximum variance** in the original feature space. Let $M$ be the number of\n",
      "data points we have in our data set. To perform linear PCA, we compute covariance matrix $T$\n",
      "\n",
      "$$\\mathbf{T} = \\frac{1}{M} \\sum_{c = i}^M \\mathbf{x_c}^\\intercal \\mathbf{x_c}$$\n",
      "\n",
      "and then compute the largeset $d'$ eigenvectors of $T$. Let $W$ be the matrix whose columns are these $d'$ eigenvectors, and so $W$ is a linear transformation from original feature space $\\mathbf{X}$ to new feature space $\\mathbf{X'}$ Then to get our reduced feature vectors, we compute\n",
      "\n",
      "$$\\mathbf{X'} = \\mathbf{X} \\mathbf{T}$$\n",
      "\n",
      "where the rows of $\\mathbf{X}$ are our original feature vectors $\\mathbf{x_c}$ and the rows of $\\mathbf{X'}$ are our reduced feature vectors $\\mathbf{x'_c}$.\n",
      "\n",
      "Linear PCA only works well when the original data set is linearly separable. However, nonlinearly separable data can be separated if we project our original data set into a larger space, and then perform linear PCA. To do this, we define a mapping \n",
      "\n",
      "$\\Phi(\\mathbf{x_i})$ where $\\Phi : \\mathbb{R}^d \\to \\mathbb{R}^p$ where $p$ is possibly infinite.\n",
      "\n",
      "Now instead of computing the covariance matrix $T$ directly, we compute the covariance matrix $T'$ of the transformed state space:\n",
      "\n",
      "$$\\mathbf{T}' = \\frac{1}{M} \\sum_{c = i}^M \\Phi(\\mathbf{x_c})^\\intercal \\Phi(\\mathbf{x_c})$$\n",
      "\n",
      "Define a **kernel function** $K(\\mathbf{x_i}, \\mathbf{x_j}) = \\Phi(\\mathbf{x_i})^\\intercal \\Phi(\\mathbf{x_j})$. Thus, using an appropriate kernel, we can project our feature space into a much larger, possibly infinite, state space and perform PCA in that space, all without needing to ever actually perform the mapping into this huge space!\n",
      "\n",
      "This is useful for us, because we have no reason to expect that the features produced by our bag of words model and n-gram model to be linearly separable. However, using an appropriate kernel, we can project our feature vectors into a much larger space where they **are** linearly separable, and then reduce the dimensionality from there.\n",
      "\n",
      "We experiment with both linear and non-linear kernels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KernelPCAReduction(AbstractReduction):\n",
      "    \"\"\"\n",
      "    Use kernel PCA to reduce dimensionality\n",
      "\n",
      "    http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, n_components, **kwargs):\n",
      "        self.pca = KernelPCA(n_components=n_components, **kwargs)\n",
      "        self.n_components = n_components\n",
      "\n",
      "    def n_components(self):\n",
      "        return self.n_components\n",
      "\n",
      "    def fit(self, X, Y=None):\n",
      "        self.pca.fit(X)\n",
      "\n",
      "    def transform(self, X):\n",
      "        return self.pca.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To predict the upvotes, we use supervised learning models to learn a regresser that takes as input feature vectors $x_c$ and normalized upvotes $y_c$ for all comments $c$. We abstracted this functionality in the abstract base class `AbstractLearner`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abc import ABCMeta, abstractmethod\n",
      "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "import numpy as np\n",
      "\n",
      "class AbstractLearner(object):\n",
      "    \"\"\"\n",
      "    Interface for all learning models. Extend this class to create new learners.\n",
      "    \"\"\"\n",
      "    __metaclass__ = ABCMeta\n",
      "\n",
      "    @abstractmethod\n",
      "    def train(self, X, Y):\n",
      "        \"\"\"\n",
      "        Trains our model.\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        X : feature matrix. Each row is a feature vector of a datum\n",
      "            (e.g. feature vector for a comment)\n",
      "        Y : value vector. Each entry corresponds to the value associated\n",
      "            with the corresponding row in X.\n",
      "\n",
      "        num rows of X == length of Y\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Does NOT return anything. train() will have side effects on the object\n",
      "        and maintain state so that predict() can be called.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Predicts the values for the feature matrix X. This MUST be called\n",
      "        after train().\n",
      "\n",
      "        Args\n",
      "        ----\n",
      "        X : feature matrix. Each row is a feature vector of a datum\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Y : predicted values for each row of X\n",
      "        \"\"\"\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4a. Gaussian Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For our first pass at predicting upvotes, we use a **Gaussian Naive Bayes** model. This model makes the **naive bayes** assumption. Given random variable $Y$ and features $F_1, F_2, F_d$, the probability that we observe $Y = y$ given feature vector $x = (f_1, f_2, ... f_d)$, the naive bayes assumption assumes\n",
      "\n",
      "$$P(F_1 = f_1, F = f_2, ..., F = f_d | Y = y) = P(F_1 = f_1 | Y = y) \\cdots P(F_d = f_d | Y = y)$$\n",
      "\n",
      "in which we assume all the features are conditionally independent of each other, given the label value $Y$.\n",
      "\n",
      "In our example our features and label valuse (upvotes) are all continuous, and so we use the continuous form of the naive bayes assumption on the probability density functions:\n",
      "\n",
      "$$p(f_1, f2, ..., f_d | y) = p(f_1 | y) \\cdots p(f_2 | y).$$\n",
      "\n",
      "For Gaussian Naive Bayes, we assume that our probability density functions for each feature all have Gaussian distributions. To produce a regresser, we compute the predicted value $\\hat{y}$ for test vector $x$ by:\n",
      "\n",
      "$$\\hat{y} = \\arg \\max_{y} \\, p(y | x) = \\arg \\max_{y} \\, p(y) p(x | y) = \\arg \\max_{y} \\, p(y) \\prod_{i = 1}^d p(x_d | y).$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GaussianNBLearner(AbstractLearner):\n",
      "    \"\"\"\n",
      "    Gaussian Naive Bayes Learner\n",
      "\n",
      "    http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
      "\n",
      "    We need to use X.toarray() because those functions expect dense arrays.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.nb = GaussianNB()\n",
      "\n",
      "    def train(self, X, Y):\n",
      "        if hasattr(X, 'toarray'):\n",
      "            self.nb.fit(X.toarray(), Y)\n",
      "        else:\n",
      "            self.nb.fit(X, Y)\n",
      "\n",
      "    def predict(self, X):\n",
      "        if (hasattr(X, \"toarray\")):\n",
      "            return self.nb.predict(X.toarray())\n",
      "        else:\n",
      "            return self.nb.predict(X)\n",
      "\n",
      "    def score(self, X, Y):\n",
      "        return np.mean(np.abs(self.nb.predict(X) - np.array(Y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4b. Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another type of supervised learning model we used is **Support Vector Machines (SVM)**, which attempts to find a decision boundary that maximizes the distance between each point and the boundary. We use kernel SVM to allow us to experiment both linear and non-linear decision boundaries, as we have no reason to expect that our data set is linearly separable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SVMLearner(AbstractLearner):\n",
      "    \"\"\"\n",
      "    Support Vector Machine Learner for regression (continuous\n",
      "    labels as opposed to discrete labels)\n",
      "\n",
      "    http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        self.svr = SVR(**kwargs)\n",
      "\n",
      "    def train(self, X, Y):\n",
      "        self.svr.fit(X, Y)\n",
      "\n",
      "    def predict(self, X):\n",
      "        return self.svr.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4c. K Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also used **K Nearest Neighbors** which computes the nearest neighbors for every point in the feature space, and then interpolates to compute the predicted upvotes $y$ for a given test vector $x$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KNeighborsLearner(AbstractLearner):\n",
      "    \"\"\"\n",
      "    Learner using k-nearest neighbors\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        self.knn = KNeighborsRegressor(**kwargs)\n",
      "\n",
      "    def train(self, X, Y):\n",
      "        self.knn.fit(X, Y)\n",
      "\n",
      "    def predict(self, X):\n",
      "        return self.knn.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Putting it all together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}