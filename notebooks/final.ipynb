{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TODO Title\n",
      "## A CS109/AC209 Project by Ali Nahm, Joseph Ong, Kenny Yu, and R.J. Aquino\n",
      "\n",
      "TODO image\n",
      "\n",
      "## Introduction\n",
      "Reddit (http://reddit.com) is a highly trafficked site, with thousands of posts per day. Each post has an associated comment thread, and users can vote comments up or down, generating a net score, or \"karma\", for each comment. Reddit users aspire to collect this \"karma.\" When reading a comment thread, however, it is often unclear why some comments bubble to the top of the thread (with a high net score) and others fall to the bottom. In our project, we characterized the voting behavior of Reddit users, scraping (TODO HOWMANY) comments from the top posts on Reddit and then analyzing them. We built an analytical model based on our exploration, to predict how well an unscored comment would perform. Additionally, we built a predictive model that would generate a high scoring comment using just a \"start\" word. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Overview and Motivation:\n",
      "Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Related Work:\n",
      "Anything that inspired you, such as a paper, a web site, or something we discussed in class.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Initial Questions:\n",
      "What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data:\n",
      "Source, scraping method, cleanup, etc. \n",
      "TODO BY R.J: Discuss existing dataset, why it was not usable by us\n",
      "TODO BY JOSEPH\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exploratory Analysis:\n",
      "What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions?\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from operator import itemgetter\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "\n",
      "stopwords = [\"a\",\"about\",\"above\",\"across\",\"after\",\"afterwards\",\"again\",\"against\",\"all\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"amoungst\",\"amount\",\"an\",\"and\",\"another\",\"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anywhere\",\"are\",\"around\",\"as\",\"at\",\"back\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\"bill\",\"both\",\"bottom\",\"but\",\"by\",\"call\",\"can\",\"cannot\",\"cant\",\"co\",\"computer\",\"con\",\"could\",\"couldnt\",\"cry\",\"de\",\"describe\",\"detail\",\"do\",\"done\",\"down\",\"due\",\"during\",\"each\",\"eg\",\"eight\",\"either\",\"eleven\",\"else\",\"elsewhere\",\"empty\",\"enough\",\"etc\",\"even\",\"ever\",\"every\",\"everyone\",\"everything\",\"everywhere\",\"except\",\"few\",\"fifteen\",\"fify\",\"fill\",\"find\",\"fire\",\"first\",\"five\",\"for\",\"former\",\"formerly\",\"forty\",\"found\",\"four\",\"from\",\"front\",\"full\",\"further\",\"get\",\"give\",\"go\",\"had\",\"has\",\"hasnt\",\"have\",\"he\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"however\",\"hundred\",\"i\",\"ie\",\"if\",\"in\",\"inc\",\"indeed\",\"interest\",\"into\",\"is\",\"it\",\"its\",\"itself\",\"keep\",\"last\",\"latter\",\"latterly\",\"least\",\"less\",\"ltd\",\"made\",\"many\",\"may\",\"me\",\"meanwhile\",\"might\",\"mill\",\"mine\",\"more\",\"moreover\",\"most\",\"mostly\",\"move\",\"much\",\"must\",\"my\",\"myself\",\"name\",\"namely\",\"neither\",\"never\",\"nevertheless\",\"next\",\"nine\",\"no\",\"nobody\",\"none\",\"noone\",\"nor\",\"not\",\"nothing\",\"now\",\"nowhere\",\"of\",\"off\",\"often\",\"on\",\"once\",\"one\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"part\",\"per\",\"perhaps\",\"please\",\"put\",\"rather\",\"re\",\"same\",\"see\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"serious\",\"several\",\"she\",\"should\",\"show\",\"side\",\"since\",\"sincere\",\"six\",\"sixty\",\"so\",\"some\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhere\",\"still\",\"such\",\"system\",\"take\",\"ten\",\"than\",\"that\",\"the\",\"their\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"thereupon\",\"these\",\"they\",\"thick\",\"thin\",\"third\",\"this\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"top\",\"toward\",\"towards\",\"twelve\",\"twenty\",\"two\",\"un\",\"under\",\"until\",\"up\",\"upon\",\"us\",\"very\",\"via\",\"was\",\"we\",\"well\",\"were\",\"what\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"would\",\"yet\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reading the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We began with the cleaned data files, as discussed above, which contained JSON records, one per line. We loaded each subreddit file into its own Pandas Dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FIELDS = [\"body\", \"post_ups\", \"subreddit_id\", \"created\", \"downs\",\n",
      "          \"author\", \"post_net\", \"subreddit\", \"post_id\", \"post_downs\",\n",
      "          \"net\", \"ups\", \"id\", \"post_created\"]\n",
      "\n",
      "def load_subreddit(filename, fields=FIELDS):\n",
      "    \"\"\"\n",
      "    Loads the subreddit with the filename and returns\n",
      "    a dataframe where the column names are the fields\n",
      "    in the json object.\n",
      "    \"\"\"\n",
      "    file = open(filename, \"rb\")\n",
      "    arrays = {field:[] for field in fields}\n",
      "    for line in file.readlines():\n",
      "        data = json.loads(line)\n",
      "        for field in fields:\n",
      "            arrays[field].append(data[field])\n",
      "    df = pd.DataFrame(arrays)\n",
      "    file.close()\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "science_df = load_subreddit(\"../data/science\")\n",
      "science_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>author</th>\n",
        "      <th>body</th>\n",
        "      <th>created</th>\n",
        "      <th>downs</th>\n",
        "      <th>id</th>\n",
        "      <th>net</th>\n",
        "      <th>post_created</th>\n",
        "      <th>post_downs</th>\n",
        "      <th>post_id</th>\n",
        "      <th>post_net</th>\n",
        "      <th>post_ups</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>subreddit_id</th>\n",
        "      <th>ups</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>          kylev</td>\n",
        "      <td> i  for one  am tired of not being able to acce...</td>\n",
        "      <td> 1232995419</td>\n",
        "      <td> 40</td>\n",
        "      <td> c07a9sl</td>\n",
        "      <td> 221</td>\n",
        "      <td> 1232994138</td>\n",
        "      <td> 319</td>\n",
        "      <td> 7sjqp</td>\n",
        "      <td> 3387</td>\n",
        "      <td> 3706</td>\n",
        "      <td> science</td>\n",
        "      <td> t5_mouw</td>\n",
        "      <td> 261</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>      DemonWasp</td>\n",
        "      <td> this  exactly i do n't want to see the blogspa...</td>\n",
        "      <td> 1232996639</td>\n",
        "      <td>  8</td>\n",
        "      <td> c07aae4</td>\n",
        "      <td>  57</td>\n",
        "      <td> 1232994138</td>\n",
        "      <td> 319</td>\n",
        "      <td> 7sjqp</td>\n",
        "      <td> 3387</td>\n",
        "      <td> 3706</td>\n",
        "      <td> science</td>\n",
        "      <td> t5_mouw</td>\n",
        "      <td>  65</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>        nbloomf</td>\n",
        "      <td>                                      hal abelson </td>\n",
        "      <td> 1232997318</td>\n",
        "      <td>  1</td>\n",
        "      <td> c07aaq0</td>\n",
        "      <td>   0</td>\n",
        "      <td> 1232994138</td>\n",
        "      <td> 319</td>\n",
        "      <td> 7sjqp</td>\n",
        "      <td> 3387</td>\n",
        "      <td> 3706</td>\n",
        "      <td> science</td>\n",
        "      <td> t5_mouw</td>\n",
        "      <td>   1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> WhirlingVortex</td>\n",
        "      <td> is n't this what plos does already  a law was ...</td>\n",
        "      <td> 1232997380</td>\n",
        "      <td>  3</td>\n",
        "      <td> c07aar3</td>\n",
        "      <td>  12</td>\n",
        "      <td> 1232994138</td>\n",
        "      <td> 319</td>\n",
        "      <td> 7sjqp</td>\n",
        "      <td> 3387</td>\n",
        "      <td> 3706</td>\n",
        "      <td> science</td>\n",
        "      <td> t5_mouw</td>\n",
        "      <td>  15</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>    Virtualmatt</td>\n",
        "      <td> luckily  as a penn state student  i can access...</td>\n",
        "      <td> 1232998300</td>\n",
        "      <td>  3</td>\n",
        "      <td> c07ab57</td>\n",
        "      <td>   4</td>\n",
        "      <td> 1232994138</td>\n",
        "      <td> 319</td>\n",
        "      <td> 7sjqp</td>\n",
        "      <td> 3387</td>\n",
        "      <td> 3706</td>\n",
        "      <td> science</td>\n",
        "      <td> t5_mouw</td>\n",
        "      <td>   7</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "           author                                               body     created  downs       id  net  post_created  post_downs post_id  post_net  post_ups subreddit subreddit_id  ups\n",
        "0           kylev  i  for one  am tired of not being able to acce...  1232995419     40  c07a9sl  221    1232994138         319   7sjqp      3387      3706   science      t5_mouw  261\n",
        "1       DemonWasp  this  exactly i do n't want to see the blogspa...  1232996639      8  c07aae4   57    1232994138         319   7sjqp      3387      3706   science      t5_mouw   65\n",
        "2         nbloomf                                       hal abelson   1232997318      1  c07aaq0    0    1232994138         319   7sjqp      3387      3706   science      t5_mouw    1\n",
        "3  WhirlingVortex  is n't this what plos does already  a law was ...  1232997380      3  c07aar3   12    1232994138         319   7sjqp      3387      3706   science      t5_mouw   15\n",
        "4     Virtualmatt  luckily  as a penn state student  i can access...  1232998300      3  c07ab57    4    1232994138         319   7sjqp      3387      3706   science      t5_mouw    7"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We read in our data and took some initial counts, to see how much data we were working with."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load in all files into a \"dfs\" dictionary\n",
      "subreddits = [\"Conservative\",\"LadyBoners\",\"Liberal\",\"Music\",\"WTF\",\"aww\",\"books\",\"circlejerk\",\"food\",\"funny\",\"gaming\",\"gentlemanboners\",\"movies\",\"nba\",\"news\",\"nfl\",\"pics\",\"pokemon\",\"politics\",\"science\",\"soccer\",\"technology\",\"television\",\"videos\",\"worldnews\"]\n",
      "dfs = {}\n",
      "for subreddit in subreddits:\n",
      "    print \"Reading {}...\".format(subreddit)\n",
      "    path = \"../data/\" + subreddit\n",
      "    \n",
      "    # load df\n",
      "    df = load_subreddit(path)\n",
      "    dfs[subreddit] = df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading Conservative...\n",
        "Reading LadyBoners..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading Liberal..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading Music..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading WTF..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading aww..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading books..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading circlejerk..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading food..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading funny..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading gaming..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading gentlemanboners..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading movies..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading nba..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading news..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading nfl..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading pics..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading pokemon..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading politics..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading science..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading soccer..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading technology..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading television..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading videos..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Reading worldnews..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We read in our data, and decided to see how many comments we had for each subreddit, and what the average number of up votes and down votes were for each subreddit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for each subreddit, print out the number of comments and average up/down/net scores\n",
      "all = {\n",
      "       \"num_comments\": 0,\n",
      "       \"total_ups\": 0,\n",
      "       \"total_downs\": 0,\n",
      "       \"total_net\": 0\n",
      "}\n",
      "\n",
      "sub_counts = {}\n",
      "for sub in subreddits:\n",
      "    # compute values\n",
      "    num_comments = len(dfs[sub])\n",
      "    total_ups = dfs[sub][\"ups\"].sum()\n",
      "    total_downs = dfs[sub][\"downs\"].sum()\n",
      "    total_net = dfs[sub][\"net\"].sum()\n",
      "    \n",
      "    # store counts, in case needed later\n",
      "    sub_counts[sub] = {\n",
      "       \"num_comments\": num_comments,\n",
      "       \"total_ups\": total_ups,\n",
      "       \"total_downs\": total_downs,\n",
      "       \"total_net\": total_net\n",
      "    }\n",
      "    \n",
      "    # add numbers to global totals\n",
      "    all[\"num_comments\"] += num_comments\n",
      "    all[\"total_ups\"] += total_ups\n",
      "    all[\"total_downs\"] += total_downs\n",
      "    all[\"total_net\"] += total_net\n",
      "    \n",
      "    # print values\n",
      "    print \"/r/{}:\".format(sub)\n",
      "    print \"\\tNum Comments: {}\".format(num_comments)\n",
      "    print \"\\tAvg Ups: {}\".format(float(total_ups) / num_comments)\n",
      "    print \"\\tAvg Downs: {}\".format(float(total_downs) / num_comments)\n",
      "    print \"\\tAvg Net: {}\".format(float(total_net) / num_comments)\n",
      "\n",
      "# print global\n",
      "sub_counts[\"all\"] = all\n",
      "print \"all:\"\n",
      "print \"\\tNum Comments: {}\".format(all[\"num_comments\"])\n",
      "print \"\\tAvg Ups: {}\".format(float(all[\"total_ups\"]) / all[\"num_comments\"])\n",
      "print \"\\tAvg Downs: {}\".format(float(all[\"total_downs\"]) / all[\"num_comments\"])\n",
      "print \"\\tAvg Net: {}\".format(float(all[\"total_net\"]) / all[\"num_comments\"])\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/r/Conservative:\n",
        "\tNum Comments: 24313\n",
        "\tAvg Ups: 12.6367375478\n",
        "\tAvg Downs: 7.88072224736\n",
        "\tAvg Net: 4.75601530046\n",
        "/r/LadyBoners:\n",
        "\tNum Comments: 18025\n",
        "\tAvg Ups: 13.8434951456\n",
        "\tAvg Downs: 2.60349514563\n",
        "\tAvg Net: 11.24\n",
        "/r/Liberal:\n",
        "\tNum Comments: 4723\n",
        "\tAvg Ups: 5.66419648528\n",
        "\tAvg Downs: 2.92060131272\n",
        "\tAvg Net: 2.74359517256\n",
        "/r/Music:\n",
        "\tNum Comments: 120628\n",
        "\tAvg Ups: 21.9404781643\n",
        "\tAvg Downs: 5.23565838777\n",
        "\tAvg Net: 16.7048197765\n",
        "/r/WTF:\n",
        "\tNum Comments: 132042\n",
        "\tAvg Ups: 38.9690628739\n",
        "\tAvg Downs: 9.94510080126\n",
        "\tAvg Net: 29.0239620727\n",
        "/r/aww:\n",
        "\tNum Comments: 78615\n",
        "\tAvg Ups: 21.6792596833\n",
        "\tAvg Downs: 4.75737454684\n",
        "\tAvg Net: 16.9218851364\n",
        "/r/books:\n",
        "\tNum Comments: 55961\n",
        "\tAvg Ups: 13.2154178803\n",
        "\tAvg Downs: 3.20326656064\n",
        "\tAvg Net: 10.0121513197\n",
        "/r/circlejerk:\n",
        "\tNum Comments: 43918\n",
        "\tAvg Ups: 26.8831686325\n",
        "\tAvg Downs: 4.54558495378\n",
        "\tAvg Net: 22.3375836787\n",
        "/r/food:\n",
        "\tNum Comments: 48555\n",
        "\tAvg Ups: 10.5296261971\n",
        "\tAvg Downs: 2.91988466687\n",
        "\tAvg Net: 7.60974153022\n",
        "/r/funny:\n",
        "\tNum Comments: 98598\n",
        "\tAvg Ups: 38.3800888456\n",
        "\tAvg Downs: 9.78677052273\n",
        "\tAvg Net: 28.5933183229\n",
        "/r/gaming:\n",
        "\tNum Comments: 123015\n",
        "\tAvg Ups: 38.2126407349\n",
        "\tAvg Downs: 9.84023086615\n",
        "\tAvg Net: 28.3724098687\n",
        "/r/gentlemanboners:\n",
        "\tNum Comments: 18307\n",
        "\tAvg Ups: 14.880755995\n",
        "\tAvg Downs: 4.96433058393\n",
        "\tAvg Net: 9.91642541104\n",
        "/r/movies:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Comments: 124949\n",
        "\tAvg Ups: 34.515186196\n",
        "\tAvg Downs: 9.69618004146\n",
        "\tAvg Net: 24.8190061545\n",
        "/r/nba:\n",
        "\tNum Comments: 65164\n",
        "\tAvg Ups: 25.2712387208\n",
        "\tAvg Downs: 5.19437112516\n",
        "\tAvg Net: 20.0768675956\n",
        "/r/news:\n",
        "\tNum Comments: 127765\n",
        "\tAvg Ups: 26.6238719524\n",
        "\tAvg Downs: 7.36956913083\n",
        "\tAvg Net: 19.2543028216\n",
        "/r/nfl:\n",
        "\tNum Comments: 95589\n",
        "\tAvg Ups: 32.1884317233\n",
        "\tAvg Downs: 5.65468830096\n",
        "\tAvg Net: 26.5337434224\n",
        "/r/pics:\n",
        "\tNum Comments: 118967\n",
        "\tAvg Ups: 40.734304471\n",
        "\tAvg Downs: 10.6295779502\n",
        "\tAvg Net: 30.1047265208\n",
        "/r/pokemon:\n",
        "\tNum Comments: 44637\n",
        "\tAvg Ups: 20.7041243811\n",
        "\tAvg Downs: 4.5997042812\n",
        "\tAvg Net: 16.1044200999\n",
        "/r/politics:\n",
        "\tNum Comments: 142085\n",
        "\tAvg Ups: 27.5127142204\n",
        "\tAvg Downs: 7.32572051941\n",
        "\tAvg Net: 20.186993701\n",
        "/r/science:\n",
        "\tNum Comments: 102006\n",
        "\tAvg Ups: 18.7798560869\n",
        "\tAvg Downs: 4.74013293336\n",
        "\tAvg Net: 14.0397231535\n",
        "/r/soccer:\n",
        "\tNum Comments: 67765\n",
        "\tAvg Ups: 24.0965395115\n",
        "\tAvg Downs: 5.71173909835\n",
        "\tAvg Net: 18.3848004132\n",
        "/r/technology:\n",
        "\tNum Comments: 141061\n",
        "\tAvg Ups: 31.1403151828\n",
        "\tAvg Downs: 7.78298750186\n",
        "\tAvg Net: 23.3573276809\n",
        "/r/television:\n",
        "\tNum Comments: 50680\n",
        "\tAvg Ups: 13.2224743489\n",
        "\tAvg Downs: 3.39832280979\n",
        "\tAvg Net: 9.82415153907\n",
        "/r/videos:\n",
        "\tNum Comments: 136353\n",
        "\tAvg Ups: 43.1846677374\n",
        "\tAvg Downs: 9.18947877934\n",
        "\tAvg Net: 33.9951889581\n",
        "/r/worldnews:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Comments: 147280\n",
        "\tAvg Ups: 37.5039720261\n",
        "\tAvg Downs: 10.5979766431\n",
        "\tAvg Net: 26.9059953829\n",
        "all:\n",
        "\tNum Comments: 2131001\n",
        "\tAvg Ups: 29.7556725689\n",
        "\tAvg Downs: 7.44040148268\n",
        "\tAvg Net: 22.3152710862\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We scraped over 2 million comments across 25 subreddits. Because the top posts from each subreddit had different numbers of comments, we have a different number of comments for each. For example, /r/technology is a popular subreddit, with over 4 million subscribers. /r/Liberal, on the other hand, only has 16,000 subscribers. We have 140,000 and 5,000 comments for these two subreddits, respectfully. Thus we must be careful when considering overall statistics, as individual subreddits will be weighted differently. We continue doing the majority of our analysis on a per subreddit basis. \n",
      "\n",
      "It appears that the small subreddits, which are trafficked less frequently, have smaller absolute numbers for each count (ups, downs, and net). For this reason, when comparing subreddits, we decided to use a ratio of up votes to down votes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "up_down_ratios = []\n",
      "for sub in subreddits:\n",
      "    # compute ratio\n",
      "    ratio = float(sub_counts[sub][\"total_ups\"]) / sub_counts[sub][\"total_downs\"]\n",
      "    up_down_ratios.append(ratio)\n",
      "    \n",
      "    print \"/r/{}:\".format(sub)\n",
      "    print \"\\tUp:Down Ratio: {}\".format(ratio)\n",
      "\n",
      "# print overall stat separately \n",
      "ratio = float(sub_counts[\"all\"][\"total_ups\"]) / sub_counts[\"all\"][\"total_downs\"]\n",
      "print\n",
      "print \"Overall totals:\"\n",
      "print \"\\tUp:Down Ratio: {}\".format(ratio)\n",
      "\n",
      "print\n",
      "print \"Max Subreddit U/D Ratio:\", max(up_down_ratios)\n",
      "print \"Min Subreddit U/D Ratio:\", min(up_down_ratios)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/r/Conservative:\n",
        "\tUp:Down Ratio: 1.60349992693\n",
        "/r/LadyBoners:\n",
        "\tUp:Down Ratio: 5.31727326969\n",
        "/r/Liberal:\n",
        "\tUp:Down Ratio: 1.93939393939\n",
        "/r/Music:\n",
        "\tUp:Down Ratio: 4.19058627192\n",
        "/r/WTF:\n",
        "\tUp:Down Ratio: 3.91841808873\n",
        "/r/aww:\n",
        "\tUp:Down Ratio: 4.55697979417\n",
        "/r/books:\n",
        "\tUp:Down Ratio: 4.12560666748\n",
        "/r/circlejerk:\n",
        "\tUp:Down Ratio: 5.91412742382\n",
        "/r/food:\n",
        "\tUp:Down Ratio: 3.60617880444\n",
        "/r/funny:\n",
        "\tUp:Down Ratio: 3.92162958726\n",
        "/r/gaming:\n",
        "\tUp:Down Ratio: 3.88330733848\n",
        "/r/gentlemanboners:\n",
        "\tUp:Down Ratio: 2.99753526551\n",
        "/r/movies:\n",
        "\tUp:Down Ratio: 3.55966845174\n",
        "/r/nba:\n",
        "\tUp:Down Ratio: 4.86511997542\n",
        "/r/news:\n",
        "\tUp:Down Ratio: 3.61267687158\n",
        "/r/nfl:\n",
        "\tUp:Down Ratio: 5.69234412406\n",
        "/r/pics:\n",
        "\tUp:Down Ratio: 3.83216574185\n",
        "/r/pokemon:\n",
        "\tUp:Down Ratio: 4.50118597096\n",
        "/r/politics:\n",
        "\tUp:Down Ratio: 3.75563252072\n",
        "/r/science:\n",
        "\tUp:Down Ratio: 3.96188384396\n",
        "/r/soccer:\n",
        "\tUp:Down Ratio: 4.21877454425\n",
        "/r/technology:\n",
        "\tUp:Down Ratio: 4.00107480262\n",
        "/r/television:\n",
        "\tUp:Down Ratio: 3.89088238197\n",
        "/r/videos:\n",
        "\tUp:Down Ratio: 4.69935986299\n",
        "/r/worldnews:\n",
        "\tUp:Down Ratio: 3.53878606162\n",
        "\n",
        "Overall totals:\n",
        "\tUp:Down Ratio: 3.99920254816\n",
        "\n",
        "Max Subreddit U/D Ratio: 5.91412742382\n",
        "Min Subreddit U/D Ratio: 1.60349992693\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this very coarse overview of reddit, it appears that the subreddits perform differently, with an up:down ratio ranging from 1.6 to 5.9, and an overall average of 4.0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Final Analysis:\n",
      "What did you learn about the data? How did you answer the questions? How can you justify your answers?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exploring Reddit Data\n",
      "\n",
      "Given the "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}