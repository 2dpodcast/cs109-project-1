{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from operator import itemgetter\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "stopwords = [\"a\",\"about\",\"above\",\"across\",\"after\",\"afterwards\",\"again\",\"against\",\"all\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"amoungst\",\"amount\",\"an\",\"and\",\"another\",\"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anywhere\",\"are\",\"around\",\"as\",\"at\",\"back\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\"bill\",\"both\",\"bottom\",\"but\",\"by\",\"call\",\"can\",\"cannot\",\"cant\",\"co\",\"computer\",\"con\",\"could\",\"couldnt\",\"cry\",\"de\",\"describe\",\"detail\",\"do\",\"done\",\"down\",\"due\",\"during\",\"each\",\"eg\",\"eight\",\"either\",\"eleven\",\"else\",\"elsewhere\",\"empty\",\"enough\",\"etc\",\"even\",\"ever\",\"every\",\"everyone\",\"everything\",\"everywhere\",\"except\",\"few\",\"fifteen\",\"fify\",\"fill\",\"find\",\"fire\",\"first\",\"five\",\"for\",\"former\",\"formerly\",\"forty\",\"found\",\"four\",\"from\",\"front\",\"full\",\"further\",\"get\",\"give\",\"go\",\"had\",\"has\",\"hasnt\",\"have\",\"he\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"however\",\"hundred\",\"i\",\"ie\",\"if\",\"in\",\"inc\",\"indeed\",\"interest\",\"into\",\"is\",\"it\",\"its\",\"itself\",\"keep\",\"last\",\"latter\",\"latterly\",\"least\",\"less\",\"ltd\",\"made\",\"many\",\"may\",\"me\",\"meanwhile\",\"might\",\"mill\",\"mine\",\"more\",\"moreover\",\"most\",\"mostly\",\"move\",\"much\",\"must\",\"my\",\"myself\",\"name\",\"namely\",\"neither\",\"never\",\"nevertheless\",\"next\",\"nine\",\"no\",\"nobody\",\"none\",\"noone\",\"nor\",\"not\",\"nothing\",\"now\",\"nowhere\",\"of\",\"off\",\"often\",\"on\",\"once\",\"one\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"part\",\"per\",\"perhaps\",\"please\",\"put\",\"rather\",\"re\",\"same\",\"see\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"serious\",\"several\",\"she\",\"should\",\"show\",\"side\",\"since\",\"sincere\",\"six\",\"sixty\",\"so\",\"some\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhere\",\"still\",\"such\",\"system\",\"take\",\"ten\",\"than\",\"that\",\"the\",\"their\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"thereupon\",\"these\",\"they\",\"thick\",\"thin\",\"third\",\"this\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"top\",\"toward\",\"towards\",\"twelve\",\"twenty\",\"two\",\"un\",\"under\",\"until\",\"up\",\"upon\",\"us\",\"very\",\"via\",\"was\",\"we\",\"well\",\"were\",\"what\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"would\",\"yet\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Pre-Scraping\n",
      "\n",
      "Initially, we planned to use a data source that someone had already created by scraping Reddit new comments to reddit continuously over the course of a week. However, after running our initial machine learning algorithms, we were surprised to find that every comment seemed to score zero or one upvotes!\n",
      "\n",
      "We soon realized that this was because the comments in the data set we obtain had been scraped fresh off `r/new`. This means that they didn't have any time to accumulate upvotes, and thus, every single one of them had absolutely no useful voting metadata at all. Oops!\n",
      "\n",
      "# Data Scraping\n",
      "\n",
      "So, before processing data, we had to get our data first! \n",
      "\n",
      "In particular, the data that we sought to get was the list of comments from top 200 posts in each of 25 popular subreddits. We estimated that we would probably find something like:\n",
      "\n",
      "200 posts/subreddit \\* 25 subreddits \\* ~200 comments/post = 1 million comments.\n",
      "\n",
      "**It turns out that those posts contained 2.2 million comments in total.**\n",
      "\n",
      "# Raw Data\n",
      "\n",
      "The first challenges in doing this were dealing with the massive amount of data we decided needed to scrape for any NLP or ML to even be feasible. On top of that, the Reddit API rate limits individuals to one request every two seconds, which puts a huge bottleneck on scraping data. In summary,\n",
      "\n",
      "1. We need lots of data.\n",
      "2. Data is slow to scrape, rate limiting.\n",
      "\n",
      "In order to solve this, we did two things:\n",
      "\n",
      "1. First, we used prawtools in order to making scraping more feasible. This helps as avoid the intricacies of having to time requests so that they fit within Reddit's rate limit.\n",
      "\n",
      "2. Next, in order to circumvent rate limiting from a single machine, we used MapReduce to parallelize scraping, making requests from multiple machines. This let us scrape several subreddits in parallel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pygments import highlight\n",
      "from pygments.lexers import PythonLexer\n",
      "from pygments.formatters import HtmlFormatter\n",
      "from IPython.display import HTML\n",
      "import urllib\n",
      "thecode = open(\"../scraper/scraper/scraper.py\").read()\n",
      "thehtml = highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">from</span> <span class=\"nn\">mrjob.job</span> <span class=\"kn\">import</span> <span class=\"n\">MRJob</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">mrjob.protocol</span> <span class=\"kn\">import</span> <span class=\"n\">JSONValueProtocol</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">operator</span> <span class=\"kn\">import</span> <span class=\"n\">itemgetter</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">praw</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">itertools</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n",
        "\n",
        "<span class=\"c\"># attributes we want to save for the submission</span>\n",
        "<span class=\"n\">sattrs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&#39;domain&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;ups&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;downs&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;permalink&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;num_comments&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;created&#39;</span><span class=\"p\">,</span> \n",
        "          <span class=\"s\">&#39;downs&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;over_18&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;stickied&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;selftext&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;title&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;id&#39;</span><span class=\"p\">]</span>\n",
        "\n",
        "<span class=\"c\"># attributes we want to save for the comments, + nested attributes</span>\n",
        "<span class=\"n\">cattrs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&#39;subreddit_id&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;id&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;body&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;downs&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;ups&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;created&#39;</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">cnattrs</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s\">&#39;author&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;name&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">&#39;subreddit&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;display_name&#39;</span><span class=\"p\">)]</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">get_top_submissions</span><span class=\"p\">(</span><span class=\"n\">subreddit</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">):</span>\n",
        "    <span class=\"c\"># connect to praw, and get the top 200 submissions for this subreddit</span>\n",
        "    <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">praw</span><span class=\"o\">.</span><span class=\"n\">Reddit</span><span class=\"p\">(</span><span class=\"n\">user_agent</span><span class=\"o\">=</span><span class=\"s\">&#39;User-Agent: awesome comment vote prediction nlp project &#39;</span> <span class=\"o\">+</span> <span class=\"n\">subreddit</span> <span class=\"o\">+</span> <span class=\"s\">&#39; v1.0 by /u/jkarnage&#39;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">submissions</span> <span class=\"o\">=</span> <span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">get_subreddit</span><span class=\"p\">(</span><span class=\"n\">subreddit</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">get_top_from_all</span><span class=\"p\">(</span><span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"n\">n</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">submissions</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">get_submission_json</span><span class=\"p\">(</span><span class=\"n\">submission</span><span class=\"p\">):</span>\n",
        "    <span class=\"c\"># create the JSON object for the submission</span>\n",
        "    <span class=\"n\">sdict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">attr</span> <span class=\"ow\">in</span> <span class=\"n\">sattrs</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">sdict</span><span class=\"p\">[</span><span class=\"n\">attr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">getattr</span><span class=\"p\">(</span><span class=\"n\">submission</span><span class=\"p\">,</span> <span class=\"n\">attr</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sdict</span><span class=\"p\">[</span><span class=\"s\">&#39;net&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">sdict</span><span class=\"p\">[</span><span class=\"s\">&#39;ups&#39;</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">sdict</span><span class=\"p\">[</span><span class=\"s\">&#39;downs&#39;</span><span class=\"p\">]</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">sdict</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">get_submission_comments</span><span class=\"p\">(</span><span class=\"n\">submission</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">replace_more_comments</span><span class=\"p\">()</span>\n",
        "    <span class=\"n\">comments</span> <span class=\"o\">=</span> <span class=\"n\">praw</span><span class=\"o\">.</span><span class=\"n\">helpers</span><span class=\"o\">.</span><span class=\"n\">flatten_tree</span><span class=\"p\">(</span><span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">comments</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">comments</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">get_comment_json</span><span class=\"p\">(</span><span class=\"n\">comment</span><span class=\"p\">):</span>\n",
        "    <span class=\"c\"># create the JSON object for the comment</span>\n",
        "    <span class=\"n\">cdict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n",
        "    \n",
        "    <span class=\"c\"># regular attributes, flat access </span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">attr</span> <span class=\"ow\">in</span> <span class=\"n\">cattrs</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"n\">attr</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">getattr</span><span class=\"p\">(</span><span class=\"n\">comment</span><span class=\"p\">,</span> <span class=\"n\">attr</span><span class=\"p\">)</span>\n",
        "\n",
        "    <span class=\"c\"># attributes where we have to index twice, i.e. comment.subreddit.display_name </span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">attr</span> <span class=\"ow\">in</span> <span class=\"n\">cnattrs</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">try</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"n\">attr</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"nb\">getattr</span><span class=\"p\">(</span><span class=\"nb\">getattr</span><span class=\"p\">(</span><span class=\"n\">comment</span><span class=\"p\">,</span> <span class=\"n\">attr</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">attr</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n",
        "        <span class=\"k\">except</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"n\">attr</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"s\">&quot;[deleted]&quot;</span>\n",
        "\n",
        "    <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"s\">&#39;net&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"s\">&#39;ups&#39;</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">cdict</span><span class=\"p\">[</span><span class=\"s\">&#39;downs&#39;</span><span class=\"p\">]</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">cdict</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">class</span> <span class=\"nc\">RedditScraper</span><span class=\"p\">(</span><span class=\"n\">MRJob</span><span class=\"p\">):</span>\n",
        "  <span class=\"n\">INPUT_PROTOCOL</span>  <span class=\"o\">=</span> <span class=\"n\">JSONValueProtocol</span>\n",
        "  <span class=\"n\">OUTPUT_PROTOCOL</span> <span class=\"o\">=</span> <span class=\"n\">JSONValueProtocol</span>\n",
        "\n",
        "  <span class=\"k\">def</span> <span class=\"nf\">mapper</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">subreddit</span><span class=\"p\">):</span>\n",
        "    <span class=\"c\"># for each subreddit, run an API call to fetch the top 200 posts</span>\n",
        "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">submissions</span> <span class=\"o\">=</span> <span class=\"n\">get_top_submissions</span><span class=\"p\">(</span><span class=\"n\">subreddit</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">submission</span> <span class=\"ow\">in</span> <span class=\"n\">submissions</span><span class=\"p\">:</span> \n",
        "            <span class=\"c\"># XXX: we don&#39;t really need this for now, we can always request them later</span>\n",
        "            <span class=\"c\"># get the json for the submission, yield for later processing</span>\n",
        "            <span class=\"c\"># sjson = get_submission_json(submission)</span>\n",
        "            <span class=\"c\"># yield sjson[&quot;id&quot;], sjon</span>\n",
        "        \n",
        "            <span class=\"c\"># get json for each of a submission&#39;s comments, serialize and send to reducer</span>\n",
        "            <span class=\"k\">try</span><span class=\"p\">:</span>\n",
        "                <span class=\"n\">comments</span> <span class=\"o\">=</span> <span class=\"n\">get_submission_comments</span><span class=\"p\">(</span><span class=\"n\">submission</span><span class=\"p\">)</span>\n",
        "                <span class=\"k\">for</span> <span class=\"n\">comment</span> <span class=\"ow\">in</span> <span class=\"n\">comments</span><span class=\"p\">:</span>\n",
        "                    <span class=\"n\">cjson</span> <span class=\"o\">=</span> <span class=\"n\">get_comment_json</span><span class=\"p\">(</span><span class=\"n\">comment</span><span class=\"p\">)</span>\n",
        "                    <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;post_id&quot;</span><span class=\"p\">]</span>      <span class=\"o\">=</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">id</span>\n",
        "                    <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;post_created&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">created</span>\n",
        "                    <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;post_ups&quot;</span><span class=\"p\">]</span>     <span class=\"o\">=</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">ups</span> \n",
        "                    <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;post_downs&quot;</span><span class=\"p\">]</span>   <span class=\"o\">=</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">downs</span>\n",
        "                    <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;post_net&quot;</span><span class=\"p\">]</span>     <span class=\"o\">=</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">ups</span> <span class=\"o\">-</span> <span class=\"n\">submission</span><span class=\"o\">.</span><span class=\"n\">downs</span>\n",
        "                    <span class=\"k\">yield</span> <span class=\"n\">cjson</span><span class=\"p\">[</span><span class=\"s\">&quot;subreddit&quot;</span><span class=\"p\">],</span> <span class=\"n\">cjson</span>\n",
        "            <span class=\"k\">except</span><span class=\"p\">:</span>\n",
        "                <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">stderr</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">subreddit</span> <span class=\"o\">+</span> <span class=\"s\">&quot; submission failed</span><span class=\"se\">\\n</span><span class=\"s\">&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">except</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">stderr</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">subreddit</span> <span class=\"o\">+</span> <span class=\"s\">&quot; failed</span><span class=\"se\">\\n</span><span class=\"s\">&quot;</span><span class=\"p\">)</span>\n",
        "\n",
        "    <span class=\"c\"># keep track of the number of subreddits we&#39;ve processed</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">stderr</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">subreddit</span> <span class=\"o\">+</span> <span class=\"s\">&quot; processed</span><span class=\"se\">\\n</span><span class=\"s\">&quot;</span><span class=\"p\">)</span>\n",
        "  \n",
        "  <span class=\"k\">def</span> <span class=\"nf\">reducer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">subreddit</span><span class=\"p\">,</span> <span class=\"n\">comments</span><span class=\"p\">):</span>\n",
        "    <span class=\"c\"># sort the results by timestamp within each subreddit before outputting</span>\n",
        "    <span class=\"n\">comments</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">comments</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"s\">&#39;created&#39;</span><span class=\"p\">))</span> \n",
        "\n",
        "    <span class=\"c\"># yield the subreddit name as a demarcator in the final file</span>\n",
        "    <span class=\"k\">yield</span> <span class=\"s\">&quot;&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;&lt;--- SUBREDDIT &quot;</span> <span class=\"o\">+</span> <span class=\"n\">subreddit</span> <span class=\"o\">+</span> <span class=\"s\">&quot; ---&gt;&quot;</span>\n",
        "    \n",
        "    <span class=\"c\"># yield each of the comments on its own line</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">comment</span> <span class=\"ow\">in</span> <span class=\"n\">comments</span><span class=\"p\">:</span>\n",
        "      <span class=\"k\">yield</span> <span class=\"s\">&quot;&quot;</span><span class=\"p\">,</span> <span class=\"n\">comment</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"s\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n",
        "  <span class=\"n\">RedditScraper</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<IPython.core.display.HTML at 0x1071527d0>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# MapReduce Parameters\n",
      "\n",
      "We used 12 c1.large instances on Amazon, each handling ~2 subreddits, since a whole lot of data transferral needs to happen, and smaller machines were running out of memory.\n",
      "\n",
      "In order to reduce our memory footprint, we took only the metadata that we thought would be useful in the future for machine learning. By doing this, we were able to cut our runtime down to approximately 6 hours to scrape all 2.2 million comments.\n",
      "\n",
      "The `scraper.py` file takes in a list of subreddits as input, and spits out a large file of, one comment per line, with unique identifiers separating the comments of different subreddits.\n",
      "\n",
      "Instructions for running the scraper can be found in the README provided in the repository. Make sure you have all dependencies.\n",
      "\n",
      "# Data Processing\n",
      "\n",
      "Next, the comments data wasn't entirely clean. In particular:\n",
      "\n",
      "1. Some comments left by users have been deleted, so they don't provide good learning information.\n",
      "2. Paragraphs need to be tokenized into sentences, and sentences into words.\n",
      "3. Vocabulary-wise, the comments still include punctuation and links.\n",
      "\n",
      "To do this, we wrote another smaller MapReduce job that takes in the raw data set,\n",
      "\n",
      "1. Removes all deleted comments.\n",
      "2. Uses ntlk to tokenize the words.\n",
      "3. Used a regular expression to strip all trailing and starting punctuation.\n",
      "\n",
      "As an added benefit, the domains of any URLs were extracted from links.\n",
      "\n",
      "# Segmenting Data\n",
      "\n",
      "Finally, we needed data organized into separate files for each subreddit. We wrote a bash script (only works on GNU and not BSD version) that splits on the unique tokens we received from the MapReduce job. \n",
      "\n",
      "Finally, we have our data!"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Comments Subreddit \n",
      "   24313 Conservative\n",
      "   18025 LadyBoners\n",
      "    4723 Liberal\n",
      "  120628 Music\n",
      "  132042 WTF\n",
      "   78615 aww\n",
      "   55961 books\n",
      "   43918 circlejerk\n",
      "   48555 food\n",
      "   98598 funny\n",
      "  123015 gaming\n",
      "   18307 gentlemanboners\n",
      "  124949 movies\n",
      "   65164 nba\n",
      "  127765 news\n",
      "   95589 nfl\n",
      "  118967 pics\n",
      "   44637 pokemon\n",
      "  142085 politics\n",
      "  102006 science\n",
      "   67765 soccer\n",
      "  141061 technology\n",
      "   50680 television\n",
      "  136353 videos\n",
      "  147280 worldnews\n",
      " 2131001 total"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}