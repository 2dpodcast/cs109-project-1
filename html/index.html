<!DOCTYPE html>
<html>
  <head>
    <title>The Karma Train</title>
    <link href="css/index.css" rel="stylesheet" type="text/css"/>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>    
    <script type="text/javascript" src="//use.typekit.net/gzo3ist.js"></script>
    <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<script type="text/javascript" src="predict.js"></script>
    <script>
        // pad the last section with whitespace
        var padContentToPage = function() {
            var lastSection = $('#site-content .section').eq('-1');
            var windowHeight = $(window).height();
            var padding = $(window).height() - lastSection.height();

            if (padding > 0) {
                lastSection.css({
                    'padding-bottom': padding
                }); 
            }
        };     

        $(document).ready(function() {
            padContentToPage();
            $(window).on('resize', padContentToPage);


            var htmlString = '';
            $('.section h1').each(function(i, e) {
                if (i == 0)
                    htmlString += "<li class='selected'>" + $(e).text() + "</li>";
                else
                    htmlString += "<li>" + $(e).text() + "</li>";
            });
            $("#navigation").html(htmlString);
            $("#navigation").on('click', 'li', function() {
                var scrollTo = $('.section h1:contains(' + $(this).text() + ')');
                var scrollNew = scrollTo.position().top;
                $('#site-content-wrapper').stop().animate({
                    scrollTop: scrollNew
                }); 
            }); 

            $('#site-content-wrapper').on('scroll', function() {
                var scrollTop = $(this).scrollTop();
                var $prev = $('#navigation li').eq(0);
                $('.section h1').each(function(i, e) {
                    if ($(e).position().top > scrollTop) {
                        return false;
                    }   
                    $prev = $(this);
                }); 

                var $li = $('#navigation li:contains(' + $prev.text() + ')');
                $('#navigation li').removeClass('selected');
                $li.addClass('selected');
            }); 
        });
    </script>
  </head>
  <body>
    <div id="sidebar">
        <h1>THE KARMA TRAIN</h1>
        <p class="subtext">A LOOK INTO THE WORLD OF REDDIT</p>
        <div id="photos">
            <img src="/img/joseph.png"/>
            <a href="http://rjaquino.net"><img src="/img/rj.png"/></a>
            <a href="http://kennyyu.me"><img src="/img/kenny.png"/></a>
            <img src="/img/ali.png"/>
        </div>
        <ul id="navigation">
        </ul>
    </div>
    <div id="site-content-wrapper">
        <div id="site-content">
            <div class="section">
                <h1 class="marginless">A VIDEO SUMMARY</h1>
                <iframe width="800" height="450" src="//www.youtube.com/embed/azD8i4nJgQc?vq=hd1080" frameborder="0" allowfullscreen></iframe>
            </div>
            <div class="section">
                <h1>BACKGROUND AND MOTIVATION</h1>
                <h2>WHAT IS REDDIT?</h2>
                <p>First, a little explanation on what reddit is. Here is the definition of the website from Wikipedia:</p>
                <p class="quote">
                Reddit is a social news and entertainment website where registered users submit content in the form of links or text posts. Users then vote each submission "up" or "down" to rank the post and determine its position on the site's pages. Content entries are organized by areas of interest called "subreddits".
                </p>
                <p>
                It's not just any old voting site, though; reddit has boomed in popularity in recent years. According to their statistics <a href="http://reddit.com/about">here</a>, they receive over 5 billion page views from 90 million unique visitors a <strong>month</strong>. Further, these users vote 22 million times each <strong>day</strong>.
                <p>
                <h2>SOCIAL ANALYSIS</h2>
                <p>
                Well, this boom makes Reddit a great place for social analysis. With this many users typing comments and responses to posts, as well as voting on content which they believe is good, there's bound to be a gold mine of patterns in social interactions hidden under all that text. Our motivation here is to better understand what brings people together in online communities, and what keeps those communities together. 
                </p>
            </div>
            <div class="section">
                <h1>PROBLEM AND DATASET</h1>
                <h2>OVERVIEW</h2>
                <p>
                Given our motivation, we set out to do two things:
                </p>
                <ol>
                    <li><strong>Find interesting and defining characteristics of different subreddits.</strong></li>
                    <li>Build programs that could <strong>(a) predict what people were likely to say</strong>, or, more interestingly, <strong>(b) guess the number of upvotes a comment might get.</strong></li>
                </ol>
                <p>
                In order to achieve these goals, we used a variety of techniques, each explained in layman's terms:
                </p>
                <ol>
                    <li><strong>Word Frequency Analysis.</strong> Counting how many times words occur.</li>
                    <li><strong>N-Gram Analysis.</strong> Count how many times combinations of words occur.</li>
                    <li><strong>Time Analaysis.</strong> Analyzing how the passage of time affects voting patterns.</li>
                    <li><strong>Machine Learning.</strong> Train computers to spot patterns that lead to high vote numbers.</li>
                    <li><strong>Problem Reduction.</strong> Make the problems smaller and easier for computers to tackle.</li>
                </ol>
                <h2>DATASET, SCRAPING</h2>
                <p>But first, before we can perform analysis, we must obtain data first.</p>
                <p>Initially, we were going to use a data source we found on reddit that scraped comments continuously over the period of a week, but we soon realized that the dataset took piping fresh comments off r/new, and thus had no voting data whatsoever!</p>
                <p>What a conundrum. We then decided to scrape our own data, but ran into rate limiting issues with Reddit, where they would only let us grab data every two seconds. To solve this, we ran something called a MapReduce job, that essentially let us use multiple computers to fetch data from Reddit simultaneously, along with the wonderful <a href="https://praw.readthedocs.org/en/latest/">praw</a>, which makes interacting with the Reddit API super easy!</p>
                <p>
                    By doing this, we were able to scrape close to 2.2 million comments from Reddit, taken from the 200 top posts of 25 popular subreddits. 
                </p>
                <p>
                    We then cleaned punctuation, lowercased words, and removed "user-deleted" comments, resulting in the follow dataset, where the number of comments is on the left, and the subreddit is on the right.
                </p>
                <img src="/img/scraped.jpg"/>
            </div>
            <div class="section">
                <h1>DATA ANALYSIS</h1>
                <h2>GENERAL WORD FREQUENCY ANALYSIS</h2>
                <p>In order to explore the data we had collected, we decided to compute different metrics of the comments.
                
                To begin, we computed the following metrics for all rows in our dataframes:
                
                <ol>
                    <li>Number of words in a comment</li>
                    <li>Length of comment (in characters)</li>
                    <li>Average word length in a comment (in characters)</li>
                    <li>Number of letters in a comment</li>
                    <li>Number of swear words in a comment</li>
                </ol>
                <p>
                Our next task was to determine whether any of these metric correlated with upvotes. Below are a few graphs displaying these metrics on all comments in a particular subreddit.
                </p>
                <img src="/img/aww_swear.png">
                <img src="/img/funny_word_length.png">

                <p>
                We determined that these metrics would not be terribly useful as features, as they span the possible Y values.
                </p>

                <p>
                The next features we looked at related to the posts that the comments were on. We wondered whether comments on posts with many comments had more upvotes than comments on posts without many comments. Below is a graph over the entire dataset, comparing comments per post vs net upvotes.
                </p>
                <img src="/img/combined_comments_per_post.png">
                <p>
                Again, while there is an upward trend for the max at any X, the values still span the Y values, and is therefore not a terribly useful feature. 
                
                <h2>Time Analysis</h2>
                <p>
                Next we took a look at vote decay with time - how well do comments that are posted early perform compared to those that are posted late? Below we show the number of comments by time and the average net vote count as a function of time, grouping comments by the half-hour.
                </p>
                <img src="/img/number_of_comments_by_time.png">
                <p class="caption">The number of posts on a thread peaks four hours after the post is created.</p>
                <img src="/img/vote_decay.png">
                <p class="caption">
                We found that comments posted in the first half-hour performed best, with scores quickly decaying after that.
                </p>
                <h2>SPECIFIC WORD FREQUENCY ANALYSIS</h2>
                <p>
                Finally, we explored certain word frequency counts and their impact on upvotes. As a cute example, here's a graph of the top 10 animal words in /r/aww, and the average upvotes comments containing those words receive.
                </p>
                <img src="/img/animal-graph.jpg" alt="animal image here">
                <h2>OTHER FUN STATISTICS</h2>
                <p>
                In our initial survey of the data, we noted some fun trends that we decided to include here. For instance, the comment with the most swear words in /r/soccer had 7 swears, whereas /r/nfl had a comment with 65 swears! 
                </p> 
                <p>On other ends of the spectrum r/Liberal maxed out at only 5, and <strong>r/circlejerk at ... 236.</strong></p>
                </p>
                <img src="/img/curses.jpg">
                <p>
                /r/Liberal and /r/Conservative seem to have longer and explanatory comments, with an average comment length of about 50 words. /r/aww and /r/funny both average only around 20 words per comment. 
                </p>
                <p>
                While statistics like these didn't ultimately impact our learning results, it did encourage us to work with each subreddit separately, as they clearly have different characteristics and commenting behavior.
                </p>
                
            </div>
            <div class="section">
                <h1>PREDICTING UPVOTES</h1>
                <h2>Overview</h2>
                <p>Next, we tried something harder. <strong>How might you predict the number of votes a comment will get?</strong> To do this, we had to process the data in four steps. Here's what they mean:</p>
                <ol>
                    <li><strong>Feature Selection.</strong> How do you represent a comment to the computer? As a collection of words? Of phrases? As a collection of topics it related to? These collections are "features".</li>
                    <li><strong>Dimensionality Reduction.</strong> These representations are big. How do you make them fit nicely on a computer, which has limited space?</li>
                    <li><strong>Supervised Learning.</strong> Teach the computer how to notice which features make for a good or bad comment, based on how those features have historically performed.</li>
                    <li><strong>Prediction.</strong> Finally, test the computer after you've taught it what to do!</li>
                </ol>
                <p>If you're interested, our specifics are below.</p>
                <h2>TECHNICALITIES</h2>
                <img src="/img/upvotes_process.png">
                <p>
                    We use the comments and upvotes data within a subreddit to build a predictor on how many votes a comment will receive on a given subreddit. We treat this problem as an example of supervised learning, where we want to perform a regression from feature vectors on text onto an upvote score. To do this, we separated the process into multiple parts.
                </p>
                <ol>
                    <li>
                        <strong>Upvote Normalization.</strong> In order to provide input to our supervised learning models, we normalized our upvotes to the interval <code>[0,1]</code></li>
                    <li>
                        <strong>Feature Selection.</strong> We used different types of models to extract features from text in subreddit comments. These features include:
                        <ul>
                            <li>Bag of Words</li>
                            <li>N-gram model</li>
                            <li>Latent Dirichlet Allocation to discover <i>topics</i> in text.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Dimensionality Reduction.</strong> Because the dimensionality of the feature space can be extremely huge (on the order of tens of thousands of features for a small data set), we needed to reduce the dimensionality of the feature space to make the training phase computationally tractable. To do this, we used multiple reduction models, including:
                        <ul>
                            <li>Kernel Principle Components Analysis</li>
                            <li>Selecting the K best features</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Learning.</strong> We used several supervised learning techniques to build a regressor from our feature space <code>X</code> to our normalized upvotes <code>Y</code>. Our supervised learning methods include:
                        <ul>
                            <li>Gaussian Naive Bayes</li>
                            <li>Support Vector Machines</li>
                            <li>K Nearest Neighbors</li>
                            <li>Decision Trees</li>
                        </ul>
                    </li>
                    <li><strong>Putting it all together.</strong> We experimented with different combinations of feature selection, dimensionality reduction, and learning models, and we measured the error for each of these combinations through cross validation.</li>
                </ol>
                <p>If you're interested in learning more, you can find read up on most of them at <a href="http://scikit-learn.org/stable/">scikit-learn</a>.</p>
                <h2>Results</h2>
                <p>
                Below are our cross validation training and test errors for various combinations of (feature model, reducer model, learning model). We defined the <strong>error</strong> of a feature vector <code>x</code> with predicted normalized upvotes <code>y'</code> and true normalized upvotes <code>y</code> as <code>|y' - y|</code>. 
                </p>
                <p>
                <strong>In other words, our metric for performance was how close our model got to predicting the right number of votes on comments we'd seen before.</strong>
                </p>
                <p>    
                The error of a data set is the sum of all the errors of individual feature vectors <code>x</code>. Overall, we conclude that the biggest impact on the error of our data set was the feature model we chose--given the model, the target dimension size, the reduction model, and learning model had less impacts on the error.
                </p>
                <img src="/img/ngram_error.png"> 
                <p><strong>Ngram error.</strong> This plot shows how the error changes as we increase N for our Ngram model. We hold the reducer (select 5000 best features) and learner (naive bayes) constant.</p>
                <img src="/img/lda_error.png">
                <p><strong>LDA error.</strong> This plot shows how the error changes as we increase the number of topics discovered by our Latent Dirichlet Model. For these experiments, we did not use a reducer, and we used naive bayes as our learner.</p>
                <img src="/img/dim_error.png">
                <p><strong>Reduced dimensionality error.</strong> This plot shows how the error changes as increase the reduced dimensionality size. Here, we use 6-grams, naive bayes, and we use select k best as our reducer model.</p>
                <img src="/img/reducer_error.png">
                <p><strong>Error by Reduction Model.</strong> This plot shows how the error changes as we change our reducer model. We use 6grams with 1000 as our reduced dimension size, and we use naive bayes. We used select k best, PCA using a linear kernel, and PCA using the cosine kernel.</p>
                <img src="/img/learner_error.png">
                <p><strong>Error by Learning Model.</strong> This plot shows how the error changes as we change our learner. We tested these learners: naive bayes, SVM using a linear kernel, SVM using a radial basis function kernel, SVM using a polynomial kernel (degree 3), K nearest neighbors, and decision trees. We test all of these against (6 gram, select best 100 features) and (LDA with 1000 topics, no reducer).</p>
                <p>You can test this predictor below. Results will probably vary from video footage we have, since the training data and parameters may have changed since then.</p>
            </div>
            <div class="section paddingless" id="upvote-predictor">
                <h1>UPVOTE PREDICTOR APP</h1>
                <textarea placeholder="TYPE YOUR COMMENT HERE..." type="text" id="comment"></textarea><div id="upvote-wrapper">
                    <div id="comment-result">0</div>
                    <img class="alien" src="/img/alien.jpg">
                    <div id="upvote-subreddit">r/aww</div>
                    <div id="upvote-button">PREDICT!</div>
                </div>
            </div>
             <div class="section">
                <h1>SENTENCE PREDICTION</h1>
                <h2>HOW WE DID IT</h2>
                <p>
                    Using N-grams, we can build a predictor for the next word "a" in a sentence, given the current word "b", by seeing which word most often follows "b" in our data. 
                </p>
                <p>
                    Technically speaking, we compute the sum <code>x'</code> of all feature vectors <code>x</code>. The <code>j</code>th component of this vector represents how important ngram <code>j</code> is in the subreddit. Sort these ngrams based on their scores in decreasing order.
                </p>
                <p>
                    Now given a word <code>w</code>, we find all ngrams starting with word <code>w</code>, pick the ngram with the highest score of these ngrams, and return the next word in that ngram. To add some nondeterminism to this, we can pick the next word after word <code>w</code> using a multinomial distribution, where the probability vector for the next word is given by the normalized scores of the ngrams whose first word is <code>w</code>. As an example of sentences built with this predictor, see below (we used 6-grams on the <code>/r/Liberal</code> subreddit, max sentence length 20):
                </p>
                <h2>Sentence Prediction Examples</h2>
                <p>
                    r/liberal
                </p>
                <ul>
                    <li>liberal atheist hang casey</li>
                    <li>liberal conservative taliban fundamentalist theocratic rule important understand tyranny possibly talking points rest coming people grow number people outraged walmart order</li>
                    <li>liberals retarded babies adults believe argue claimed zimmerman facial</li>
                    <li>liberals dislike paul thought obama mccain policies corrupt self employed corporate america bashers fact government rights way local level massive government</li>
                    <li>republican behavior pattern fits cult disfavor indebting public infrastructure taxes defined supreme court finances</li>
                    <li>republican bubble separate legislation constitution supported slavery fought slavery officiate eternally valid saying obama favorite rushisms http www huffingtonpost com 42</li>
                    <li>republicans funny pullman kind hoping succeeded agreement rest fox long time cat broke law convince idiots like example provide documented proof
                    <li>republicans funny used snap better served npr podcasts mainly applied small checks registration card check lanes example forcible assault rifle straight</li>
                </ul>
                <p>
                    r/television
                </p>
                <ul>
                    <li>benedict cumberbatch says yeah baby</li>
                </ul>
                <p>
                    r/worldnews
                </p>
                <ul>
                    <li>nsa hates hope</li>
                </ul>
                <p>
                    r/circlejerk
                </p>
                <ul>
                    <li>office depot office depot office depot office depot office depot ... (non-terminating)</li>
                </ul>
            </div>
            <div class="section" id="sentence-predictor">
                <h1>SENTENCE PREDICTOR APP</h1>
                <textarea placeholder="TYPE A WORD OR PARTIAL SENTENCE HERE..." type="text" id="word"></textarea>
                <div id="sentence-button">WHAT WOULD R/LIBERAL SAY NEXT?</div><div id="clear-button">RESET</div>
            </div>
            <div class="section">
                <h1>CLUSTERING COMMENTS WITHIN SUBREDDIT</h1>
                <p>One way of analyzing the comments in a subreddit is to perform <strong>unsupervised learning</strong> on feature vectors of the comments to determine which are "most similar" to one another. To do this, we employ K-means clustering, which attempts to find K means in the data set. We cluster using feature vectors built from the Ngram and LDA models.</p>
                <p><strong>In essence, this partitions comments into groups of topics -- this can be used to tell us the main conversations that occur in any given subreddit.</strong></p>
                <p>See below for examples of clusters we found (most common words in these clusters and their counts) and a human readable name we gave to the cluster. These examples were generated on the <code>/r/Liberal</code> dataset using 1-grams with 10000 as our reduced dimension size, and creating 8 clusters:</p>
                <h2>Cluster Examples</h2>
                <p><strong>"Wage Increase." 83 items.</strong> wage 208, minimum 177, would 91, people 73, increase 51, cost 49, money 45, workers 45, wages 39, pay 37, business 35, prices 35, goods 35, jobs 34, less 33, raising 33, price 32, inflation 32, raise 32, economy 30, much 29, make 29, increases 28, time 27, companies 26, level 26, making 26</p>
                <p><strong>"Trayvon Martin Case." 124 items.</strong> good 104, zimmerman 81, martin 35, evidence 16, thing 15, would 14, one 14, trayvon 14, guy 13, george 12, like 12, said 11, know 11, luck 11, really 11, could 10, people 10, think 9, fact 9, facts 9, go 8, never 8, say 8, black 8, job 8</p>
                <p><strong>"Obamacare." 104 items.</strong> tax 133, taxes 56, would 42, income 40, people 38, rate 34, government 31, much 29, pay 28, insurance 26, money 20, think 17, want 16, health 16, even 15, spending 15, obamacare 14, actually 14, rates 14, also 14, higher 14, like 14, work 13, get 13, jobs 13, business 13</p>
            </div>
        </div>
    </div>
  </body>
</html>
