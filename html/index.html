<!DOCTYPE html>
<html>
  <head>
    <title>AC209: JANKY OAR</title>
    <!--script src="js/index.js" type="text/javascript"></script-->
    <link href="css/index.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="//use.typekit.net/gzo3ist.js"></script>
    <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
  </head>
  <body>
    <div id="sidebar">
        <h1>JANKY OAR</h1>
        <p class="subtext">A LOOK INTO THE WORLD OF REDDIT</p>
        <div id="photos">
            <img src="/img/joseph.png"/>
            <a href="http://rjaquino.net"><img src="/img/rj.png"/></a>
            <a href="http://kennyyu.me"><img src="/img/kenny.png"/></a>
            <img src="/img/ali.png"/>
        </div>
        <ul id="navigation">
            <li>A VIDEO SUMMARY</li>
            <li>MOTIVATION</li>
            <li class="selected">PROBLEM AND DATASET</li>
            <li>DATA ANALYSIS</li>
            <li>PREDICTING UPVOTES</li>
            <li>SENTENCE PREDICTION</li>             
            <li>CLUSTERING COMMENTS WITHIN SUBREDDIT</li>
        </ul>
        <ul id="predictor-links">
            <li>SENTENCE PREDICTOR</li>  
            <li>VOTE PREDICTOR</li>  
        </ul>
    </div>
    <div id="site-content-wrapper">
        <div id="site-content">
            <div class="section">
                <h1 class="marginless">A VIDEO SUMMARY</h1>
                <iframe width="800" height="450" src="//www.youtube.com/embed/lVQ1EKIyH_w" frameborder="0" allowfullscreen></iframe> 
            </div>
            <div class="section">
                <h1>BACKGROUND AND MOTIVATION</h1>
                <p>First, a little explanation on what reddit is. </p>
            </div>
            <div class="section">
                <h1>PROBLEM AND DATASET</h1>
                <h2>OVERVIEW</h2>
                <p></p>
                <h2>SCRAPING</h2>
                <p></p>
            </div>
            <div class="section">
                <h1>DATA ANALYSIS</h1>
                <p>In order to explore the data we had collected, we decided to compute different metrics of the comments.
                
                To begin, we computed the following metrics for all rows in our dataframes:
                
                <ol>
                    <li>Number of words in a comment</li>
                    <li>Length of comment (in characters)</li>
                    <li>Average word length in a comment (in characters)</li>
                    <li>Number of letters in a comment</li>
                    <li>Number of swear words in a comment</li>
                </ol>
                Our next task was to determine whether any of these metric correlated with upvotes. Below are a few graphs displaying these metrics on all comments in a particular subreddit.
                </p>
                <img src="img/aww_swear.png"></img>
                <img src="img/funny_word_length.png"></img>

                <p>
                We determined that these metrics would not be terribly useful as features, as they span the possible Y values.
                </p>

                <p>
                The next features we looked at related to the posts that the comments were on. We wondered whether comments on posts with many comments had more upvotes than comments on posts without many comments. Below is a graph over the entire dataset, comparing comments per post vs net upvotes.
                </p>
                <img src="img/combined_comments_per_post.png"></img>
                <p>
                Again, while there is an upward trend for the max at any X, the values still span the Y values, and is therefore not a terribly useful feature. Next we took a look at vote decay with time - how well do comments that are posted early perform compared to those that are posted late? Below we show the number of comments by time and the average net vote count as a function of time, grouping comments by the half-hour.
                </p>
                <img src="img/number_of_comments_by_time.png"></img>
                <img src="img/vote_decay.png"></img>
                <p>
                We found that comments posted in the first half-hour performed best, with scores quickly decaying after that.
                </p>
                <p>
                Finally, we explored word frequency counts and their impact on upvotes. Below we show a graph of the top 20 animal words in /r/aww, and the average upvotes comments containing those words receive.
                </p>
                <img alt="animal image here"></img>
                
            </div>
            <div class="section">
                <h1>PREDICTING UPVOTES</h1>
                <h2>Overview</h2>
                <img src="img/upvotes_process.png"></img>
                <p>
                    We use the comments and upvotes data within a subreddit to build a predictor on how many votes a comment will receive on a given subreddit. We treat this problem as an example of supervised learning, where we want to perform a regression from feature vectors on text onto an upvote score. To do this, we separated the process into multiple parts.
                </p>
                <ol>
                    <li>
                        <b>Upvote Normalization.</b> In order to provide input to our supervised learning models, we normalized our upvotes to the interval <code>[0,1]</code></li>
                    <li>
                        <b>Feature Selection.</b> We used different types of models to extract features from text in subreddit comments. These features include:
                        <ul>
                            <li>Bag of Words</li>
                            <li>N-gram model</li>
                            <li>Latent Dirichlet Allocation to discover <i>topics</i> in text.</li>
                        </ul>
                    </li>
                    <li>
                        <b>Dimensionality Reduction.</b> Because the dimensionality of the feature space can be extremely huge (on the order of tens of thousands of features for a small data set), we needed to reduce the dimensionality of the feature space to make the training phase computationally tractable. To do this, we used multiple reduction models, including:
                        <ul>
                            <li>Kernel Principle Components Analysis</li>
                            <li>Selecting the K best features</li>
                        </ul>
                    </li>
                    <li>
                        <b>Learning.</b> We used several supervised learning techniques to build a regressor from our feature space <code>X</code> to our normalized upvotes <code>Y</code>. Our supervised learning methods include:
                        <ul>
                            <li>Gaussian Naive Bayes</li>
                            <li>Support Vector Machines</li>
                            <li>K Nearest Neighbors</li>
                            <li>Decision Trees</li>
                        </ul>
                    </li>
                    <li><b>Putting it all together.</b> We experimented with different combinations of feature selection, dimensionality reduction, and learning models, and we measured the error for each of these combinations through cross validation.</li>
                </ol>
                <h2>Results</h2>
                <p>
                    Below are our cross validation training and test
                    errors for various combinations of (feature model, reducer model, learning model). We defined the <b>error</b> of a feature vector <code>x</code> with predicted normalized upvotes <code>y'</code> and true normalized upvotes <code>y</code> as <code>|y' - y|</code>. The error of a data set is the sum of all the errors of individual feature vectors <code>x</code>. Overall, we conclude that the biggest impact on the error of our data set was the feature model we chose--given the model, the target dimension size, the reduction model, and learning model had less impacts on the error.
                </p>
                <img src="img/ngram_error.png"></img> 
                <p><b>Ngram error.</b> This plot shows how the error changes as we increase N for our Ngram model. We hold the reducer (select 5000 best features) and learner (naive bayes) constant.</p>
                <img src="img/lda_error.png"></img>
                <p><b>LDA error.</b> This plot shows how the error changes as we increase the number of topics discovered by our Latent Dirichlet Model. For these experiments, we did not use a reducer, and we used naive bayes as our learner.</p>
                <img src="img/dim_error.png"></img>
                <p><b>Reduced dimensionality error.</b> This plot shows how the error changes as increase the reduced dimensionality size. Here, we use 6-grams, naive bayes, and we use select k best as our reducer model.</p>
                <img src="img/reducer_error.png"></img>
                <p><b>Error by Reduction Model.</b> This plot shows how the error changes as we change our reducer model. We use 6grams with 1000 as our reduced dimension size, and we use naive bayes. We used select k best, PCA using a linear kernel, and PCA using the cosine kernel.</p>
                <img src="img/learner_error.png"></img>
                <p><b>Error by Learning Model.</b> This plot shows how the error changes as we change our learner. We tested these learners: naive bayes, SVM using a linear kernel, SVM using a radial basis function kernel, SVM using a polynomial kernel (degree 3), K nearest neighbors, and decision trees. We test all of these against (6 gram, select best 100 features) and (LDA with 1000 topics, no reducer).</p>
            </div>
             <div class="section">
                <h1>SENTENCE PREDICTION</h1>
                <p>
                    Using N-grams, we can build a predictor for the next word in a sentence, given the current word. To do this, we compute the sum <code>x'</code> of all feature vectors <code>x</code>. The <code>j</code>th component of this vector represents how important ngram <code>j</code> is in the subreddit. Sort these ngrams based on their scores in decreasing order.
                </p>
                <p>
                    Now given a word <code>w</code>, we find all ngrams starting with word <code>w</code>, pick the ngram with the highest score of these ngrams, and return the next word in that ngram. To add some nondeterminism to this, we can pick the next word after word <code>w</code> using a multinomial distribution, where the probability vector for the next word is given by the normalized scores of the ngrams whose first word is <code>w</code>. As an example of sentences built with this predictor, see below (we used 6-grams on the <code>/r/Liberal</code> subreddit, max sentence length 20):
                </p>
                <h2>Sentence Prediction Examples</h2>
                <p>
                    <ul>
                        <li>liberal atheist hang casey</li>
                        <li>liberal conservative taliban fundamentalist theocratic rule important understand tyranny possibly talking points rest coming people grow number people outraged walmart order</li>
                        <li>liberals retarded babies adults believe argue claimed zimmerman facial</li>
                        <li>liberals dislike paul thought obama mccain policies corrupt self employed corporate america bashers fact government rights way local level massive government</li>
                        <li>republican behavior pattern fits cult disfavor indebting public infrastructure taxes defined supreme court finances</li>
                        <li>republican bubble separate legislation constitution supported slavery fought slavery officiate eternally valid saying obama favorite rushisms http www huffingtonpost com 42</li>
                        <li>republicans funny pullman kind hoping succeeded agreement rest fox long time cat broke law convince idiots like example provide documented proof
                        <li>republicans funny used snap better served npr podcasts mainly applied small checks registration card check lanes example forcible assault rifle straight<li>
                    </ul>
                </p>
            </div>
             <div class="section">
                <h1>CLUSTERING COMMENTS WITHIN SUBREDDIT</h1>
                <p>One way of analyzing the comments in a subreddit is to perform <b>unsupervised learning</b> on feature vectors of the comments to determine which are "most similar" to one another. To do this, we employ K-means clustering, which attempts to find K means in the data set. We cluster using feature vectors built from the Ngram and LDA models. See below for examples of clusters we found (most common words in these clusters and their counts) and a human readable name we gave to the cluster. These examples were generated on the <code>/r/Liberal</code> dataset using 1-grams with 10000 as our reduced dimension size, and creating 8 clusters:</p>
                <h2>Cluster Examples</h2>
                <p><b>"Wage Increase." 83 items.</b> wage 208, minimum 177, would 91, people 73, increase 51, cost 49, money 45, workers 45, wages 39, pay 37, business 35, prices 35, goods 35, jobs 34, less 33, raising 33, price 32, inflation 32, raise 32, economy 30, much 29, make 29, increases 28, time 27, companies 26, level 26, making 26</p>
                <p><b>"Trayvon Martin Case." 124 items.</b> good 104, zimmerman 81, martin 35, evidence 16, thing 15, would 14, one 14, trayvon 14, guy 13, george 12, like 12, said 11, know 11, luck 11, really 11, could 10, people 10, think 9, fact 9, facts 9, go 8, never 8, say 8, black 8, job 8</p>
                <p><b>"Obamacare." 104 items.</b> tax 133, taxes 56, would 42, income 40, people 38, rate 34, government 31, much 29, pay 28, insurance 26, money 20, think 17, want 16, health 16, even 15, spending 15, obamacare 14, actually 14, rates 14, also 14, higher 14, like 14, work 13, get 13, jobs 13, business 13</p>
            </div>
        </div>
    </div>
  </body>
</html>
