<!DOCTYPE html>
<html>
  <head>
    <title>AC209: JANKY OAR</title>
    <!--script src="js/index.js" type="text/javascript"></script-->
    <link href="css/index.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="//use.typekit.net/gzo3ist.js"></script>
    <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
  </head>
  <body>
    <div id="sidebar">
        <h1>JANKY OAR</h1>
        <p class="subtext">A LOOK INTO THE WORLD OF REDDIT</p>
        <div id="photos">
            <img src="/img/joseph.png"/>
            <a href="http://rjaquino.net"><img src="/img/rj.png"/></a>
            <a href="http://kennyyu.me"><img src="/img/kenny.png"/></a>
            <img src="/img/ali.png"/>
        </div>
        <ul id="navigation">
            <li>A VIDEO SUMMARY</li>
            <li>MOTIVATION</li>
            <li class="selected">PROBLEM AND DATASET</li>
            <li>DATA ANALYSIS</li>
            <li>PREDICTING UPVOTES</li>
            <li>CLUSTERING COMMENTS IN A SUBREDDIT</li>            
            <li>SENTENCE PREDICTION</li>             
        </ul>
        <ul id="predictor-links">
            <li>SENTENCE PREDICTOR</li>  
            <li>VOTE PREDICTOR</li>  
        </ul>
    </div>
    <div id="site-content-wrapper">
        <div id="site-content">
            <div class="section">
                <h1 class="marginless">A VIDEO SUMMARY</h1>
                <iframe width="800" height="450" src="//www.youtube.com/embed/lVQ1EKIyH_w" frameborder="0" allowfullscreen></iframe> 
            </div>
            <div class="section">
                <h1>BACKGROUND AND MOTIVATION</h1>
                <p>First, a little explanation on what reddit is. </p>
            </div>
            <div class="section">
                <h1>PROBLEM AND DATASET</h1>
                <h2>OVERVIEW</h2>
                <p></p>
                <h2>SCRAPING</h2>
                <p></p>
            </div>
            <div class="section">
                <h1>DATA ANALYSIS</h1>
                <p></p>
            </div>
            <div class="section">
                <h1>PREDICTING UPVOTES</h1>
                <h2>Overview</h2>
                <img src="img/upvotes_process.png"></img>
                <p>
                    We use the comments and upvotes data within a subreddit to build a predictor on how many votes a comment will receive on a given subreddit. We treat this problem as an example of supervised learning, where we want to perform a regression from feature vectors on text onto an upvote score. To do this, we separated the process into multiple parts.
                </p>
                <ol>
                    <li>
                        <b>Upvote Normalization.</b> In order to provide input to our supervised learning models, we normalized our upvotes to the interval <code>[0,1]</code></li>
                    <li>
                        <b>Feature Selection.</b> We used different types of models to extract features from text in subreddit comments. These features include:
                        <ul>
                            <li>Bag of Words</li>
                            <li>N-gram model</li>
                            <li>Latent Dirichlet Allocation to discover <i>topics</i> in text.</li>
                        </ul>
                    </li>
                    <li>
                        <b>Dimensionality Reduction.</b> Because the dimensionality of the feature space can be extremely huge (on the order of tens of thousands of features for a small data set), we needed to reduce the dimensionality of the feature space to make the training phase computationally tractable. To do this, we used multiple reduction models, including:
                        <ul>
                            <li>Kernel Principle Components Analysis</li>
                            <li>Selecting the K best features</li>
                        </ul>
                    </li>
                    <li>
                        <b>Learning.</b> We used several supervised learning techniques to build a regressor from our feature space <code>X</code> to our normalized upvotes <code>Y</code>. Our supervised learning methods include:
                        <ul>
                            <li>Gaussian Naive Bayes</li>
                            <li>Support Vector Machines</li>
                            <li>K Nearest Neighbors</li>
                            <li>Decision Trees</li>
                        </ul>
                    </li>
                    <li><b>Putting it all together.</b> We experimented with different combinations of feature selection, dimensionality reduction, and learning models, and we measured the error for each of these combinations through cross validation.</li>
                </ol>
                <h2>Results</h2>
                <p>
                    Below are our cross validation training and test
                    errors for various combinations of (feature model, reducer model, learning model). We defined the <b>error</b> of a feature vector <code>x</code> with predicted normalized upvotes <code>y'</code> and true normalized upvotes <code>y</code> as <code>|y' - y|</code>. The error of a data set is the sum of all the errors of individual feature vectors <code>x</code>. Overall, we conclude that the biggest impact on the error of our data set was the feature model we chose--given the model, the target dimension size, the reduction model, and learning model had less impacts on the error.
                </p>
                <img src="img/ngram_error.png"></img> 
                <p><b>Ngram error.</b> This plot shows how the error changes as we increase N for our Ngram model. We hold the reducer (select 5000 best features) and learner (naive bayes) constant.</p>
                <img src="img/lda_error.png"></img>
                <p><b>LDA error.</b> This plot shows how the error changes as we increase the number of topics discovered by our Latent Dirichlet Model. For these experiments, we did not use a reducer, and we used naive bayes as our learner.</p>
                <img src="img/dim_error.png"></img>
                <p><b>Reduced dimensionality error.</b> This plot shows how the error changes as increase the reduced dimensionality size. Here, we use 6-grams, naive bayes, and we use select k best as our reducer model.</p>
                <img src="img/reducer_error.png"></img>
                <p><b>Error by Reduction Model.</b> This plot shows how the error changes as we change our reducer model. We use 6grams with 1000 as our reduced dimension size, and we use naive bayes. We used select k best, PCA using a linear kernel, and PCA using the cosine kernel.</p>
                <img src="img/learner_error.png"></img>
                <p><b>Error by Learning Model.</b> This plot shows how the error changes as we change our learner. We tested these learners: naive bayes, SVM using a linear kernel, SVM using a radial basis function kernel, SVM using a polynomial kernel (degree 3), K nearest neighbors, and decision trees. We test all of these against (6 gram, select best 100 features) and (LDA with 1000 topics, no reducer).</p>
            </div>
             <div class="section">
                <h1>CLUSTERING COMMENTS IN A SUBREDDIT</h1>
                <p></p>
            </div>
             <div class="section">
                <h1>SENTENCE PREDICTION</h1>
                <p></p>
            </div>
        </div>
    </div>
  </body>
</html>
